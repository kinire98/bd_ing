{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7950a9f9-ebef-48f1-b7ef-74689ce05ae5",
   "metadata": {},
   "source": [
    "# SISTEMAS DE BIG DATA - Examen 1ª Evaluación\n",
    "\n",
    "**Instrucciones generales**\n",
    "\n",
    "1.\tTodas las sentencias deben ejecutarse desde la línea de comandos en las celdas que hay después del enunciado. No debes realizar ninguna tarea desde fuera de Jupyter.\n",
    "2.\tPuedes **añadir** todas las celdas que necesites siempre y cuando estén antes del siguiente enunciado.\n",
    "3.\tTodas las celdas **deben estar ejecutadas** y debe visualizarse el resultado de salida.\n",
    "4.\t**No es necesario documentar** las respuestas, simplemente debes hacer lo que se pide en el enunciado.\n",
    "5.\tDespués de cada parte debes insertar una **captura de pantalla** del cliente gráfico de la base de datos correspondientes donde se vea que los datos se han cargado correctamente.\n",
    "6.\tDebes entregar tanto el **notebook** (fichero `.ipynb`) como el mismo fichero convertido a **PDF** (es muy probable que si intentas convertirlo en el propio contenedor te falle por no tener instalado `pandoc`, si es así descargalo en formato `.md` o `html` y conviértelo en tu máquina física)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ee366-1158-4061-a6f1-ec4c6fde2148",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOMBRE**:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393dc33-a532-4bba-b10a-28d4f9e76d2e",
   "metadata": {},
   "source": [
    "## Contexto del escenario\n",
    "\n",
    "Has sido contratado por una fábrica inteligente que dispone de sensores de temperatura y vibración en sus máquinas críticas. La empresa necesita un sistema backend capaz de procesar los datos que llegan de los sensores en tiempo real.\n",
    "\n",
    "El sistema debe cumplir dos objetivos simultáneos:\n",
    "\n",
    "1.  **Monitorización en vivo (Dashboard):** los operarios necesitan saber el estado *actual* de cada máquina y si hay alguna alarma activa en este preciso instante. Para esto usarás **Redis**.\n",
    "2.  **Histórico para mantenimiento predictivo:** el equipo de Data Science necesita almacenar todos los datos brutos a lo largo del tiempo para entrenar modelos de IA futuros. Para esto usarás **InfluxDB**.\n",
    "\n",
    "## Los Datos de Entrada\n",
    "\n",
    "Los datos con los que vas a trabajar los tienes en el *dataset* sintético adjunto llamado `sensores.csv`. Este *dataset* contiene lecturas simuladas con las siguientes columnas:\n",
    "\n",
    "  - `timestamp`: fecha y hora del evento.\n",
    "  - `machine_id`: identificador único de la máquina.\n",
    "  - `zone`: zona de la fábrica.\n",
    "  - `temperature`: temperatura en grados Celsius.\n",
    "  - `vibration`: nivel de vibración (0-100).\n",
    "  - `lat`, `lon`: coordenadas del robot.\n",
    "  - `status`: estado reportado por la máquina (\"OK\", \"WARNING\", \"ERROR\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1bde7-5156-4b81-a5c3-4e903f190a0c",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "El desarrollo del examen debe de ser modular, con un programa principal que inicialice las conexiones a la base de datos y lea los datos del fichero y luego invocará **una función diferente para cargar cada tipo de dato** en la base de datos\n",
    "\n",
    "Es decision tuya elegir los parámetros que recibirá cada función, aunque es altamente aconsejable **no utilizar variables globales**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1c3d1-5976-44e0-8f12-b0e36ed809cd",
   "metadata": {},
   "source": [
    "## Parte A: Persistencia histórica (InfluxDB)\n",
    "\n",
    "`2 puntos`\n",
    "\n",
    "En esta parte tienes que crear un script que lea el fichero CSV facilitado y almacene los datos en una base de datos InfluxDB.\n",
    "\n",
    "Los aspectos que tienes que tener en cuenta son:\n",
    "\n",
    "  - **Bucket:** `factory_logs`\n",
    "  - **Measurement:** `maquinaria`\n",
    "  - **Requisito clave:** debes modelar correctamente los datos usando adecuadamente *tags* o *fields* según el tipo de datos. Se debe respetar el `timestamp` del datos (no usar el tiempo de ingesta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afc05b75-32a2-444d-9ff5-5463c45a1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insertados:  10000\n",
      "No insertados:  0\n"
     ]
    }
   ],
   "source": [
    "# Función que carga los datos en InfluxDB\n",
    "\n",
    "from influxdb_client.client.influxdb_client import InfluxDBClient\n",
    "from urllib3.exceptions import NewConnectionError\n",
    "from influxdb_client.client.write_api import ASYNCHRONOUS, WriteApi, WriteOptions\n",
    "from influxdb_client.client.write.point import Point\n",
    "from influxdb_client.client.exceptions import InfluxDBError\n",
    "\n",
    "ORG = \"docs\"\n",
    "INFLUX_URL = \"http://localhost:8086\"\n",
    "INFLUX_TOKEN = \"MyInitialAdminToken0==\"\n",
    "BUCKET = \"factory_logs\"\n",
    "\n",
    "def data_to_influx(data):\n",
    "    try:\n",
    "        client = InfluxDBClient(\n",
    "            url=INFLUX_URL,\n",
    "            token=INFLUX_TOKEN,\n",
    "            org=ORG\n",
    "        )\n",
    "        options = WriteOptions(write_type=ASYNCHRONOUS)\n",
    "        write_api = client.write_api(write_options=options)\n",
    "        introduced_points = 0\n",
    "        not_introduced_points = 0\n",
    "        for point in data:\n",
    "            if insert_point(point, write_api):\n",
    "                introduced_points += 1\n",
    "            else:\n",
    "                not_introduced_points += 1\n",
    "        print(\"Insertados: \", introduced_points)\n",
    "        print(\"No insertados: \", not_introduced_points)\n",
    "    except NewConnectionError:\n",
    "        print(\"Error de conexion\")\n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "def insert_point(point, write_api: WriteApi) -> bool:\n",
    "    try:\n",
    "        point = Point(\"machine_measurement\")\\\n",
    "            .tag(\"machine_id\", point[1])\\\n",
    "            .tag(\"zone\", point[2])\\\n",
    "            .tag(\"lat\", point[5])\\\n",
    "            .tag(\"lon\", point[6])\\\n",
    "            .tag(\"status\", point[7])\\\n",
    "            .field(\"temperature\", point[3])\\\n",
    "            .field(\"vibration\", point[4])\\\n",
    "            .time(point[0])\n",
    "        write_api.write(record=point, bucket=BUCKET, org=ORG)\n",
    "        return True\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    except InfluxDBError as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def read_csv():\n",
    "    contents = []\n",
    "    with open(\"./telemetria_agv.csv\") as file:\n",
    "        text = file.read()\n",
    "    first = True\n",
    "    for row in text.split(\"\\n\"):\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        if len(row) < 8:\n",
    "            continue\n",
    "        date = True\n",
    "        row_content = []\n",
    "        content = row.split(\",\")\n",
    "        date = \"T\".join(content[0].split())\n",
    "        date += \"Z\"\n",
    "        machine_id = content[1]\n",
    "        zone = content[2]\n",
    "        temperature = float(content[3])\n",
    "        vibration = float(content[4])\n",
    "        lat = float(content[5])\n",
    "        lon = float(content[6])\n",
    "        status = content[7]\n",
    "        contents.append([date, machine_id, zone, temperature, vibration, lat, lon, status])\n",
    "    return contents\n",
    "\n",
    "\n",
    "data_to_influx(read_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16b7f5",
   "metadata": {},
   "source": [
    "**Resultado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed5349-3dab-4524-8061-ef99218adcbc",
   "metadata": {},
   "source": [
    "## Parte B - Analítica en tiempo real con Redis\n",
    "\n",
    "Debes crear un script que alimente las siguientes estructuras en Redis por cada dato procesado:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc31d35-9b8d-4c0e-9a3f-8f89f437ddcd",
   "metadata": {},
   "source": [
    "### 1.- Estadísticas agregadas\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Al procesar masivamente datos de telemetría, es costoso consultar la base de datos histórica (InfluxDB) para preguntas simples como \"¿Cuál ha sido la temperatura máxima hoy en el Almacén A?\". Vamos a usar Redis Hashes para mantener un marcador actualizado de estadísticas por zona.\n",
    "\n",
    "Para cada fila procesada del CSV, debes actualizar un Hash correspondiente a la Zona (zone) donde se encuentra el robot.\n",
    "\n",
    "- **Clave:** `stats:zone:{nombre_zona}` (Ej: stats:zone:Almacen_A, stats:zone:Recepcion...).\n",
    "- **Campos:**:\n",
    "    - `total_lecturas`: contador total de datos recibidos de esa zona.\n",
    "    - `total_errores`: contador de cuántas veces el status ha sido \"ERROR\".\n",
    "    - `max_temp`: La temperatura más alta registrada hasta el momento en esa zona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "422b5288-dcf7-4cd9-a886-d44f639ac796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que genera las estadísticas agregadas\n",
    "import redis\n",
    "BASE_KEY = \"stats:zone:\"\n",
    "READ_KEY = \"total_reads\"\n",
    "ERROR_KEY = \"total_errors\"\n",
    "TEMP_KEY = \"max_temp\"\n",
    "def insert_zone_stats(zone: str, total_reads: int, total_errors: int, max_temp: float, client: redis.Redis):\n",
    "    client.hset(f\"{BASE_KEY}{zone}\", key=READ_KEY, value=total_reads)\n",
    "    client.hset(f\"{BASE_KEY}{zone}\", key=ERROR_KEY, value=total_errors)\n",
    "    client.hset(f\"{BASE_KEY}{zone}\", key=TEMP_KEY, value=max_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03f286-7c4a-4e9e-bb24-0d10e4a3107e",
   "metadata": {},
   "source": [
    "### 2.- Ranking de \"puntos calientes\" (Sorted Set)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "El jefe de planta quiere ver en una pantalla un \"Top de Máquinas con mayor temperatura\" ordenado de mayor a menor en tiempo real.\n",
    "\n",
    "- **Estructura:** `Sorted Set` (ZSET)\n",
    "- **Clave:** `dashboard:hottest_machines`\n",
    "- **Score:** La temperatura actual (`temperature`).\n",
    "- **Member:** El ID de la máquina (`machine_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ee87d30-521d-4635-8c97-32abf50680c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga el sorted set\n",
    "import redis\n",
    "SORTED_KEY = \"dashboard:hottest_machines\"\n",
    "\n",
    "def dashboard_hottest(machine_id: int, temperature: float, client: redis.Redis):\n",
    "    client.zadd(SORTED_KEY, {machine_id: temperature})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddfd7b-b7a5-4c34-b9ef-40cae6419d07",
   "metadata": {},
   "source": [
    "### 3.- Seguimiento de flota (Geospatial)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Las máquinas de este escenario son AGVs (robots móviles) que se mueven por la planta. Necesitamos saber su ubicación exacta.\n",
    "\n",
    "- **Estructura:** `Geo`\n",
    "- **Clave:** `factory:map`\n",
    "- **Datos:** Usa la latitud y longitud que vienen en el CSV para posicionar el `machine_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2ecc26b-134c-4f0e-b72e-742c2059128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos geoespaciales\n",
    "import redis\n",
    "GEO_KEY = \"factory:map\"\n",
    "\n",
    "def add_coordinates(machine_id: int, lat: float, lon: float, client: redis.Redis):\n",
    "    client.geoadd(GEO_KEY, (lon, lat, machine_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afa47-8719-4cd0-9c26-921de0349f5b",
   "metadata": {},
   "source": [
    "### 4.- Contadores globales atómicos (String)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Necesitamos estadísticas rápidas que no requieran contar filas en una base de datos histórica.\n",
    "\n",
    "- **Estructura:** `String` (Contador)\n",
    "- **Clave:** `stats:total_processed` -\\> Incrementar en 1 por cada fila procesada.\n",
    "- **Clave:** `stats:total_errors` -\\> Incrementar en 1 solo si el `status` es \"ERROR\".\n",
    "- **Clave:** `stats:total_warnings` -\\> Incrementar en 1 solo si el `status` es \"WARNING\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c47fc3d1-cd1f-4e77-8226-0e0bd571fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que gestiona los contadores\n",
    "import redis\n",
    "PROCESSED_KEY = \"stats:total_processed:\"\n",
    "ERRORS_KEY = \"stats:total_errors\"\n",
    "WARNINGS_KEY = \"stats:total_warnings\"\n",
    "\n",
    "def manage_counter(status: str, client: redis.Redis):\n",
    "    if status == \"ERROR\":\n",
    "        client.incrby(ERRORS_KEY, 1)\n",
    "    elif status == \"WARNING\":\n",
    "        client.incrby(WARNINGS_KEY, 1)\n",
    "    client.incrby(PROCESSED_KEY, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193de2fc-b8ed-4bfc-9bb6-a7c8e6aec991",
   "metadata": {},
   "source": [
    "### 5.- Cola de anomalías críticas (List)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Queremos tener también una cola de anomalías críticas. Por cada registro cuyo `status` sea `ERROR` deberás crear un JSON y almacenarlo en una estructura tipo FIFO:\n",
    "\n",
    "- **Estructura:** `List`\n",
    "- **Clave:** `alerts:queue`\n",
    "- **Datos:**: el JSON debe incluir: `machine_id`, `timestamp` y un mensaje: *\"Critical failure at [Lat, Lon]\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81917494-a1c2-4dba-b4fe-64d6595b3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos en la cola\n",
    "import redis\n",
    "import json\n",
    "ERROR_LIST_QUEUE = \"alerts:queue\"\n",
    "\n",
    "def insert_alert(machine_id: int, timestamp: str, lat: float, lon: float, client: redis.Redis):\n",
    "    json_data = json.dumps({\n",
    "        \"machine_id\": machine_id,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"message\": f\"Critical failure at: [{lat},{lon}]\"\n",
    "    })\n",
    "    client.lpush(ERROR_LIST_QUEUE, json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df4799-0663-4c9e-bb15-590e9c97441b",
   "metadata": {},
   "source": [
    "## Programa principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac019ce5-4388-4667-89c6-99f5f75bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí debes insertar el programa principal que llama al resto de funciones\n",
    "import redis\n",
    "csv_data = read_csv()\n",
    "zones = dict()\n",
    "r = redis.Redis(\n",
    "        host=\"localhost\",\n",
    "        port=6379, \n",
    "        db=0,\n",
    "        decode_responses=True\n",
    "        )\n",
    "READS_KEY_DICT = \"total_reads\"\n",
    "ERRORS_KEY_DICT = \"total_reads\"\n",
    "MAX_TEMP_KEY_DICT = \"max_temp\"\n",
    "for row in csv_data:\n",
    "    manage_counter(row[-1], r)\n",
    "    if row[-1] == \"ERROR\":\n",
    "        insert_alert(row[1], row[0], row[5], row[6], r)\n",
    "    dashboard_hottest(row[1], row[3], r)\n",
    "    add_coordinates(row[1], row[5], row[6], r)\n",
    "    if zones.get(row[2]):\n",
    "        zones[row[2]][READS_KEY_DICT] = zones[row[2]].get(READS_KEY_DICT, 1) + 1\n",
    "        if row[-1] == \"ERROR\":\n",
    "            zones[row[2]][ERRORS_KEY_DICT] = zones[row[2]].get(ERRORS_KEY_DICT, 1) + 1\n",
    "        if row[3] > zones[row[2]][MAX_TEMP_KEY_DICT]:\n",
    "            zones[row[2]][MAX_TEMP_KEY_DICT] = row[3]\n",
    "    else:\n",
    "        zones[row[2]] = {\n",
    "            READS_KEY_DICT: 1,\n",
    "            ERRORS_KEY_DICT: 1 if row[-1] == \"ERROR\" else 0,\n",
    "            MAX_TEMP_KEY_DICT: row[3]\n",
    "        }\n",
    "for key, values in zones.items():\n",
    "    insert_zone_stats(key, values[READS_KEY_DICT], values[ERRORS_KEY_DICT], values[MAX_TEMP_KEY_DICT], r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514dfa1-6f76-4b14-86e3-dce3ae6ded91",
   "metadata": {},
   "source": [
    "## Capturas de pantalla\n",
    "\n",
    "A partir de aquí tienes que insertar las capturas de pantalla correspondientes a cada punto. Las capturas de pantalla corresponderán a la interfaz gráfica de la base de datos correspondiente y se debe mostrar que los datos se han cargado correctamente. Los apartados que no tengan la captura de pantalla correspondiente **se considerarán no realizados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45932542-a53f-4672-9680-3a735fd9d580",
   "metadata": {},
   "source": [
    "### Captura de InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fad5c6-b160-4f5a-8d8a-832bdb21c0b3",
   "metadata": {},
   "source": [
    "![](./influx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea0c97a-6181-4257-972f-52d2915a1e77",
   "metadata": {},
   "source": [
    "### Captura de estadísticas agregadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9dd4a-a7af-4e75-9274-3cc13ec7ae55",
   "metadata": {},
   "source": [
    "![](./estadisticas_agregadas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab967f49-1886-4bee-953b-61a20f4bcb5b",
   "metadata": {},
   "source": [
    "### Captura de ranking de puntos calientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215c849-1568-4e2a-adaa-8534bac7d105",
   "metadata": {},
   "source": [
    "![](./hottes_machines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06cfcb-7677-425d-bc63-7e85042d8c62",
   "metadata": {},
   "source": [
    "### Captura de seguimiento de flota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d81eb",
   "metadata": {},
   "source": [
    "En Redis las coordenadas se guarda como longitud y luego latitud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97575442-6c6f-4309-bd64-ddb670c2ff8d",
   "metadata": {},
   "source": [
    "![](./geospatial_good.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bae22-0d26-4f38-9bcb-db3eec184f8c",
   "metadata": {},
   "source": [
    "### Captura de contadores globales atómicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3747246-e0af-47c6-9509-eaf2d0f20527",
   "metadata": {},
   "source": [
    "![](./atomic_counters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a301380-6041-411e-9289-f15b97ceac37",
   "metadata": {},
   "source": [
    "### Captura de cola de anomalías críticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e594d85",
   "metadata": {},
   "source": [
    "![](./alerts.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
