{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd27e298",
   "metadata": {},
   "source": [
    "# PR0503. Limpieza de datos sobre el dataset de cultivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e4041c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('Crop', StringType(), False),\n",
       " StructField('Region', StringType(), False),\n",
       " StructField('Soil_Type', StringType(), False),\n",
       " StructField('Soil_pH', DoubleType(), False),\n",
       " StructField('Rainfall_mm', DoubleType(), False),\n",
       " StructField('Temperature_C', DoubleType(), False),\n",
       " StructField('Humidity_pct', DoubleType(), False),\n",
       " StructField('Fertilizar_Used_kg', DoubleType(), False),\n",
       " StructField('Irrigation', StringType(), False),\n",
       " StructField('Pesticides_Used_kg', DoubleType(), False),\n",
       " StructField('Planting_Density', DoubleType(), False),\n",
       " StructField('Previous_Crop', StringType(), False),\n",
       " StructField('Yield_ton_per_ha', DoubleType(), False)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "schema = StructType([\n",
    "    StructField(\"Crop\", StringType(), False),\n",
    "    StructField(\"Region\", StringType(), False),\n",
    "    StructField(\"Soil_Type\", StringType(), False),\n",
    "    StructField(\"Soil_pH\", DoubleType(), False),\n",
    "    StructField(\"Rainfall_mm\", DoubleType(), False),\n",
    "    StructField(\"Temperature_C\", DoubleType(), False),\n",
    "    StructField(\"Humidity_pct\", DoubleType(), False),\n",
    "    StructField(\"Fertilizar_Used_kg\", DoubleType(), False),\n",
    "    StructField(\"Irrigation\", StringType(), False),\n",
    "    StructField(\"Pesticides_Used_kg\", DoubleType(), False),\n",
    "    StructField(\"Planting_Density\", DoubleType(), False),\n",
    "    StructField(\"Previous_Crop\", StringType(), False),\n",
    "    StructField(\"Yield_ton_per_ha\", DoubleType(), False),\n",
    "])\n",
    "schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe269fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/22 10:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = ( SparkSession.builder\n",
    "            .appName(\"pruebas\")\n",
    "            .master(\"spark://spark-master:7077\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "df = (\n",
    "    spark.read\n",
    "        .format(\"csv\")\n",
    "        .schema(schema)\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"/workspace/pr0501/crop_yield_dataset.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da73bfe",
   "metadata": {},
   "source": [
    "## 1. Creación de un ID único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a940f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+------------------+\n",
      "|  Crop|  Region|Soil_Type|Soil_pH|Rainfall_mm|Temperature_C|Humidity_pct|Fertilizar_Used_kg|Irrigation|Pesticides_Used_kg|Planting_Density|Previous_Crop|Yield_ton_per_ha|           Crop_ID|\n",
      "+------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+------------------+\n",
      "| Maize|Region_C|    Sandy|   7.01|     1485.4|         19.7|        40.3|             105.1|      Drip|              10.2|            23.2|         Rice|          101.48| CODIGO_-XXC-MAIZE|\n",
      "|Barley|Region_D|     Loam|   5.79|      399.4|         29.1|        55.4|             221.8| Sprinkler|              35.5|             7.4|       Barley|          127.39|CODIGO_-XXD-BARLEY|\n",
      "|  Rice|Region_C|     Clay|   7.24|      980.9|         30.5|        74.4|              61.2| Sprinkler|              40.0|             5.1|        Wheat|           68.99|  CODIGO_-XXC-RICE|\n",
      "| Maize|Region_D|     Loam|   6.79|     1054.3|         26.4|        62.0|             257.8|      Drip|              42.7|            23.7|         None|          169.06| CODIGO_-XXD-MAIZE|\n",
      "| Maize|Region_D|    Sandy|   5.96|      744.6|         20.4|        70.9|             195.8|      Drip|              25.5|            15.6|        Maize|          118.71| CODIGO_-XXD-MAIZE|\n",
      "+------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lit, split, upper, lpad\n",
    "df_eng = df.withColumn(\"Crop_ID\", concat_ws(\"-\", lit(\"CODIGO_\"), lpad(split(col(\"Region\"), \"_\")[1], 3, \"X\"), upper(col(\"Crop\"))))\n",
    "df_eng.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9e37c",
   "metadata": {},
   "source": [
    "## 2. Transformación matemática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e099d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+------------------+------------------+\n",
      "|  Crop|  Region|Soil_Type|Soil_pH|Rainfall_mm|Temperature_C|Humidity_pct|Fertilizar_Used_kg|Irrigation|Pesticides_Used_kg|Planting_Density|Previous_Crop|Yield_ton_per_ha|           Crop_ID|      Log_Rainfall|\n",
      "+------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+------------------+------------------+\n",
      "| Maize|Region_C|    Sandy|   7.01|     1485.4|         19.7|        40.3|             105.1|      Drip|              10.2|            23.2|         Rice|          101.48| CODIGO_-XXC-MAIZE|3.1721356966495664|\n",
      "|Barley|Region_D|     Loam|   5.79|      399.4|         29.1|        55.4|             221.8| Sprinkler|              35.5|             7.4|       Barley|          127.39|CODIGO_-XXD-BARLEY| 2.602494068807281|\n",
      "|  Rice|Region_C|     Clay|   7.24|      980.9|         30.5|        74.4|              61.2| Sprinkler|              40.0|             5.1|        Wheat|           68.99|  CODIGO_-XXC-RICE|2.9920672600276665|\n",
      "| Maize|Region_D|     Loam|   6.79|     1054.3|         26.4|        62.0|             257.8|      Drip|              42.7|            23.7|         None|          169.06| CODIGO_-XXD-MAIZE|3.0233759381395626|\n",
      "| Maize|Region_D|    Sandy|   5.96|      744.6|         20.4|        70.9|             195.8|      Drip|              25.5|            15.6|        Maize|          118.71| CODIGO_-XXD-MAIZE| 2.872505899345925|\n",
      "+------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log10, round\n",
    "df_eng = (\n",
    "    df_eng \n",
    "    .withColumn(\"Log_Rainfall\", log10(col(\"Rainfall_mm\") + 1))\n",
    "    .withColumn(\"Yield_ton_per_ha\", round(col(\"Yield_ton_per_ha\"), 2))\n",
    "        )\n",
    "df_eng.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e857a960",
   "metadata": {},
   "source": [
    "## 3. Comparación de insumos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45deec71",
   "metadata": {},
   "source": [
    "## 4. Simulación de fechas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
